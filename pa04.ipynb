{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5da71ea4320977ef41026862c02457d5",
     "grade": false,
     "grade_id": "cell-741f65363882bd77",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Camera calibration & 3D reconstruction\n",
    "\n",
    "***Maximum possible score = 100 points***\n",
    "\n",
    "### Assignment Overview\n",
    "This assignment has been broadly divided into two sections: \n",
    "In the first part, two images of the same object, taken from a pair of calibrated cameras, will be used to triangulate the 3D coordinates of points on the object. In the second part, two smartphone cameras will be calibrated using a set of chessboard patterns.\n",
    "\n",
    "### Learning Objectives \n",
    "* Describe the basic principles of triangulation and epipolar geometry; reconstruct 3D scene structures from two images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f23339f4c3a8532f8449b0820e1bf363",
     "grade": false,
     "grade_id": "cell-67ff037347487f76",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Table of Contents \n",
    "\n",
    "### [Part 1: 3D Reconstruction [70 pts]](#part-1)\n",
    "- [Eight point algorithm [30 pts]](#q1)\n",
    "- 3D Scene Structure\n",
    "    - [Essential Matrix [20 pts]](#q2)\n",
    "    - [Epipolar Correspondence [0 pts]](#q3)\n",
    "    - [Triangulation [20 pts]](#q4)\n",
    "\n",
    "### [Part 2: Camera calibration and 3D reconstruction [30 pts]](#part-2)\n",
    "- [Checkerboard pattern detection and Camera Calibration [15 pts]](#q5)\n",
    "- [Feature descriptor matching and Fundamental Matrix estimation [15 pts]](#q6)\n",
    "\n",
    "### [Part 3: Challenge Questions](#part-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "196121d789cd7058acd2bc19017cc1ce",
     "grade": false,
     "grade_id": "cell-e9f54f125ec14067",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"part-1\"></a>\n",
    "## 3D reconstruction\n",
    "\n",
    "For this section, we will use two images from the popular [Middlebury dataset](https://vision.middlebury.edu/stereo/data/). Let's first load them and then do a side-by-side comparison of the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Change this path to point to the root of the project.\n",
    "# Nothing else needs to be changed\n",
    "root = Path(\".\").absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "im1 = io.imread(root / \"data\" / \"temple1.png\")\n",
    "im2 = io.imread(root / \"data\" / \"temple2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(im1)\n",
    "ax2.imshow(im2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8a3100b89f8268158c46fef0d520e849",
     "grade": false,
     "grade_id": "cell-e40f847c51b05b60",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "While the two images look similar, if you point to the same location on the object in the two images, you will see a slight difference in the coordinates of the two points. This is more evident when you go back and forth between the two images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "87119a397cc3636eb27a8dd56964df30",
     "grade": false,
     "grade_id": "cell-d61a3d82693d48f7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "For this part of the assignment we provide you a set of point correspondences between the two images in the form of a `npz` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "points = np.load(root / \"data\" / \"temple_corresp.npz\")\n",
    "points1 = points[\"pts1\"]\n",
    "points2 = points[\"pts2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b0f1064c782aa023e1137305917eea92",
     "grade": false,
     "grade_id": "cell-952ecf3d82f5a82b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Lets try to visualize the points on the two images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(im1)\n",
    "ax2.imshow(im2)\n",
    "cmap = plt.cm.get_cmap(\"hsv\", len(points1))\n",
    "ax1.scatter(points1[:,0], points1[:,1], s=3, c=[cmap(i) for i in range(len(points1))])\n",
    "ax2.scatter(points2[:,0], points2[:,1], s=3, c=[cmap(i) for i in range(len(points2))])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "91c8394584fb294242869ff7ec03b7a7",
     "grade": false,
     "grade_id": "cell-a328db6ba79a9f5b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q1\"></a>\n",
    "## Eight point algorithm [30 points]\n",
    "Let us now implement the 8-point algorithm we learned in class.\n",
    "\n",
    "You may find it helpful to refine the solution by using local minimization. This probably won't fix a completely broken solution, but may make a good solution better by locally minimizing a geometric cost function.\n",
    "\n",
    "We provide a helper function `refineF` that will take in `F` and two sets of points, which you can call from `eightpoint` before unscaling `F`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize\n",
    "\n",
    "def _singularize(F):\n",
    "    U, S, V = np.linalg.svd(F)\n",
    "    S[-1] = 0\n",
    "    F = U.dot(np.diag(S).dot(V))\n",
    "    return F\n",
    "\n",
    "def _objective_F(f, pts1, pts2):\n",
    "    F = _singularize(f.reshape([3, 3]))\n",
    "    num_points = pts1.shape[0]\n",
    "    hpts1 = np.concatenate([pts1, np.ones([num_points, 1])], axis=1)\n",
    "    hpts2 = np.concatenate([pts2, np.ones([num_points, 1])], axis=1)\n",
    "    Fp1 = F.dot(hpts1.T)\n",
    "    FTp2 = F.T.dot(hpts2.T)\n",
    "\n",
    "    r = 0\n",
    "    for fp1, fp2, hp2 in zip(Fp1.T, FTp2.T, hpts2):\n",
    "        r += (hp2.dot(fp1))**2 * (1 / (fp1[0]**2 + fp1[1]**2) + 1 /\n",
    "                                  (fp2[0]**2 + fp2[1]**2))\n",
    "    return r\n",
    "\n",
    "def refineF(F, pts1, pts2):\n",
    "    f = scipy.optimize.fmin_powell(lambda x: _objective_F(x, pts1, pts2),\n",
    "                                   F.reshape([-1]),\n",
    "                                   maxiter=100000,\n",
    "                                   maxfun=10000,\n",
    "                                   disp=False)\n",
    "    return _singularize(f.reshape([3, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0b9c5d9a43745c3a7970393061447453",
     "grade": false,
     "grade_id": "cell-ae5f2ab960a42208",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Define a function `eightpoint` that takes in 3 arguments &mdash; `pts1`, `pts2`, `M` where `pts1` and `pts2` are $N \\times 2$ matrices corresponding to the $(x, y)$ coordinates of the N points in the first and second image respectively, and `M` is a scale parameter.\n",
    "\n",
    "**Hint 1:** Use the scale parameter to scale down/normalize the point coordinates, find the $\\boldsymbol{F}$ matrix using SVD and then scale the data back to undo the effect of normalization &mdash;\n",
    "$$ \\boldsymbol{x}_{normalized} = \\boldsymbol{T}\\boldsymbol{x}$$\n",
    "$$ \\boldsymbol{F}_{unnormalized} = \\boldsymbol{T}^T\\boldsymbol{F}\\boldsymbol{T} $$\n",
    "where $\\boldsymbol{T}$ is a $3 \\times 3$ diagonal matrix formed from `M`.\n",
    "\n",
    "**Hint 2:** You must enforce the singularity condition of $\\boldsymbol{F}$ before unscaling it. You may use the `_singularize` helper function provided above appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2c31b2225e818dfb3ddd995a4a592da8",
     "grade": false,
     "grade_id": "cell-f80d3b3cb2189dfc",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def eightpoint(pts1, pts2, M):\n",
    "    # normalize the coordinates\n",
    "    x1, y1 = pts1[:, 0], pts1[:, 1]\n",
    "    x2, y2 = pts2[:, 0], pts2[:, 1]\n",
    "    x1, y1, x2, y2 = x1 / M, y1 / M, x2 / M, y2 / M\n",
    "    \n",
    "    # normalization matrix\n",
    "    T = np.array([[1. / M, 0, 0], [0, 1. / M, 0], [0, 0, 1]])\n",
    "\n",
    "    A = np.transpose(\n",
    "        np.vstack((x2 * x1, x2 * y1, x2, y2 * x1, y2 * y1, y2, x1, y1,\n",
    "                   np.ones(x1.shape))))\n",
    "\n",
    "    # get F using SVD decomposition\n",
    "    _, _, vh = np.linalg.svd(A) #TODO\n",
    "    f=vh[-1, :] #TODO\n",
    "    F = np.reshape(f, (3, 3))\n",
    "\n",
    "    # refine F\n",
    "    F = refineF(F, pts1 / M, pts2 / M)\n",
    "\n",
    "    # constraint of rank 2 by setting the last singular value to 0\n",
    "    F = _singularize(F)\n",
    "\n",
    "    # rescale the data\n",
    "    F = np.dot(np.transpose(T), np.dot(F, T))\n",
    "\n",
    "    return F\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1ea80a3be39afab82f2206693b5cd433",
     "grade": true,
     "grade_id": "cell-9ff30c2138258e8d",
     "locked": true,
     "points": 30,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "43def002f4a59fb103afe62cb54998c6",
     "grade": false,
     "grade_id": "cell-76daba69590b00ea",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Below we define some variables that will become useful later in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_points = np.load(root / \"data\" / \"temple_corresp.npz\")\n",
    "_pts1 = _points[\"pts1\"]\n",
    "_pts2 = _points[\"pts2\"]\n",
    "_im1 = io.imread(root / \"data\" / \"temple1.png\")\n",
    "_im2 = io.imread(root / \"data\" / \"temple2.png\")\n",
    "_M = np.max(_im1.shape)\n",
    "_x1, _y1 = _pts1[:, 0], _pts1[:, 1]\n",
    "_x2, _y2 = _pts2[:, 0], _pts2[:, 1]\n",
    "_x1s, _y1s, _x2s, _y2s = _x1 / _M, _y1 / _M, _x2 / _M, _y2 / _M\n",
    "_T = np.array([[1. / _M, 0, 0], [0, 1. / _M, 0], [0, 0, 1]])\n",
    "_As = np.vstack((_x2s * _x1s, _x2s * _y1s, _x2s, _y2s * _x1s, _y2s * _y1s, _y2s, _x1s, _y1s,\n",
    "               np.ones(_x1s.shape))).T\n",
    "_, _, _vhs = np.linalg.svd(_As)\n",
    "_fs = _vhs[-1, :]\n",
    "_Fs = np.reshape(_fs, (3, 3))\n",
    "_Frefs = refineF(_Fs, _pts1 / _M, _pts2 / _M)\n",
    "_FSings = _singularize(_Frefs)\n",
    "_FSingCorrs = np.dot(np.transpose(_T), np.dot(_FSings, _T))\n",
    "\n",
    "_A = np.vstack((_x2 * _x1, _x2 * _y1, _x2, _y2 * _x1, _y2 * _y1, _y2, _x1, _y1,\n",
    "               np.ones(_x1.shape))).T\n",
    "_, _, _vh = np.linalg.svd(_A)\n",
    "_f = _vh[-1, :]\n",
    "_F = np.reshape(_f, (3, 3))\n",
    "_Fref = refineF(_F, _pts1 / _M, _pts2 / _M)\n",
    "_FSing = _singularize(_Fref)\n",
    "\n",
    "F8 = eightpoint(_pts1, _pts2, _M)\n",
    "tol = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "85adc040ade56d29ad5619fbe9a5fe8e",
     "grade": false,
     "grade_id": "cell-d0216798630f8ed2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let us visualize the $\\boldsymbol{F}$ matrix by plotting points on the first image and the corresponding epipolar line in the second image using the fundamental matrix $\\boldsymbol{F}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayEpipolarF(I1, I2, F, x, y):\n",
    "    sy, sx, _ = I2.shape\n",
    "\n",
    "    f, [ax1, ax2] = plt.subplots(1, 2)\n",
    "    f.suptitle(\"Verify that the corresponding point \\n is on the epipolar line\")\n",
    "    ax1.imshow(I1)\n",
    "    ax1.set_title(\"Point in image1\")\n",
    "    ax1.set_axis_off()\n",
    "    ax2.imshow(I2)\n",
    "    ax2.set_title(\"Epipolar line in image2\")\n",
    "    ax2.set_axis_off()\n",
    "\n",
    "    for xc, yc in zip(x, y):\n",
    "        v = np.array([xc, yc, 1])\n",
    "        l = F.dot(v)\n",
    "        s = np.sqrt(l[0]**2 + l[1]**2)\n",
    "\n",
    "        assert s != 0, \"Zero line vector in displayEpipolar\"\n",
    "        \n",
    "        _l = l / s\n",
    "        if _l[0] != 0:\n",
    "            ye = sy - 1\n",
    "            ys = 0\n",
    "            xe = -(l[1] * ye + _l[2]) / _l[0]\n",
    "            xs = -(l[1] * ys + _l[2]) / _l[0]\n",
    "        else:\n",
    "            xe = sx - 1\n",
    "            xs = 0\n",
    "            ye = -(_l[0] * xe + _l[2]) / _l[1]\n",
    "            ys = -(_l[0] * xs + _l[2]) / _l[1]\n",
    "\n",
    "        ax1.plot(xc, yc, '*', markersize=6, linewidth=2)\n",
    "        ax2.plot([xs, xe], [ys, ye], linewidth=2)\n",
    "        plt.draw()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayEpipolarF(im1, im2, F8, [54, 135, 186, 423, 522], [183, 209, 355, 128, 231])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1427fcd41b610709544fe5e6072216b",
     "grade": false,
     "grade_id": "cell-c1275ace5c345226",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### 3D Scene Structure\n",
    "\n",
    "Let us now compute the camera matrices and triangulate the 2D points to obtain the 3D scene\n",
    "structure. To obtain the Euclidean scene structure, first convert the fundamental matrix $\\boldsymbol{F}$\n",
    "to an essential matrix $\\boldsymbol{E}$. The internal camera calibration matrices $\\boldsymbol{K}_1$ and \n",
    "$\\boldsymbol{K}_2$ are known; these are provided in `data/intrinsics.npz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic = np.load('./data/temple_intrinsics.npz')\n",
    "K1, K2 = intrinsic['K1'], intrinsic['K2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "22f30f9b88e3c1f671657d86eb982be0",
     "grade": false,
     "grade_id": "cell-d3bb16bb89eb33ec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Given an essential matrix, it is possible to retrieve the projective camera matrices $\\boldsymbol{M}_1$ and\n",
    "$\\boldsymbol{M}_2$ from it. Assuming $\\boldsymbol{M}_1$ is fixed at $[\\boldsymbol{I}, 0]$, $\\boldsymbol{M}_2$ can be retrieved up to a scale and four-fold rotation ambiguity.\n",
    "\n",
    "We have provided you with the function `camera2` to recover the four possible $\\boldsymbol{M}_2$ matrices given $\\boldsymbol{E}$.\n",
    "Note: The $\\boldsymbol{M}_1$ and $\\boldsymbol{M}_2$ here are projection matrices of the form: $\\boldsymbol{M}_1 = [\\boldsymbol{I}|0]$ and $\\boldsymbol{M}_2 = [\\boldsymbol{R}|\\boldsymbol{t}]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camera2(E):\n",
    "    U, S, V = np.linalg.svd(E)\n",
    "    m = S[:2].mean()\n",
    "    E = U.dot(np.array([[m, 0, 0], [0, m, 0], [0, 0, 0]])).dot(V)\n",
    "    U, S, V = np.linalg.svd(E)\n",
    "    W = np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]])\n",
    "\n",
    "    if np.linalg.det(U.dot(W).dot(V)) < 0:\n",
    "        W = -W\n",
    "\n",
    "    return np.dstack([\n",
    "        np.concatenate(\n",
    "            [U.dot(W).dot(V), U[:, 2].reshape([-1, 1]) / abs(U[:, 2]).max()],\n",
    "            axis=1),\n",
    "        np.concatenate(\n",
    "            [U.dot(W).dot(V), -U[:, 2].reshape([-1, 1]) / abs(U[:, 2]).max()],\n",
    "            axis=1),\n",
    "        np.concatenate(\n",
    "            [U.dot(W.T).dot(V), U[:, 2].reshape([-1, 1]) / abs(U[:, 2]).max()],\n",
    "            axis=1),\n",
    "        np.concatenate([\n",
    "            U.dot(W.T).dot(V), -U[:, 2].reshape([-1, 1]) / abs(U[:, 2]).max()\n",
    "        ],\n",
    "                       axis=1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d57c76c2ed4f960613da3010a87c7620",
     "grade": false,
     "grade_id": "cell-60163535a0ec6eaa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q2\"></a>\n",
    "### Essential Matrix [20 points]\n",
    "\n",
    "Write a function to compute the essential matrix $\\boldsymbol{E}$ given $\\boldsymbol{F}$, $\\boldsymbol{K}_1$ and $\\boldsymbol{K}_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5aba331f7600aabe78820c3ce6c6a41a",
     "grade": false,
     "grade_id": "cell-1997b160cc9454f9",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def essentialMatrix(F, K1, K2):\n",
    "    E = np.dot(np.transpose(K2), np.dot(F, K1))\n",
    "    return E\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "25d7357a3b0eca58349b9e137cb536e0",
     "grade": true,
     "grade_id": "cell-5df0e127e573f120",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dadafc626cac72a7e05d976a92797c6c",
     "grade": false,
     "grade_id": "cell-27794d86273758c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q3\"></a>\n",
    "#### Epipolar correspondence [0 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.load('./data/temple_coords.npz')\n",
    "x1, y1 = coords['x1'][:, 0], coords['y1'][:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1a48dc92f264804e00338ffcbdd68335",
     "grade": false,
     "grade_id": "cell-a40dd047f882df50",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to create a 3D visualization of the temple images, we will treat our two images as\n",
    "a stereo-pair. We can then triangulate corresponding points in each image and render their 3D\n",
    "locations.\n",
    "\n",
    "Variables `x1` and `y1` contain the $x$ and $y$ coordinates of a single 2D point in the first image that\n",
    "we want to 3D reconstruct.\n",
    "\n",
    "We want to implement a function that takes in the $x$ and $y$ coordinates of a pixel in the first image and the\n",
    "fundamental matrix $F$, and returns the coordinates of the pixel in the second image which corresponds\n",
    "to the input point. The match is obtained by computing the similarity of a small window around the\n",
    "$(x_1, y_1)$ coordinates in `im1` to various windows around possible matches in `im2` and returning the\n",
    "closest.\n",
    "\n",
    "Instead of searching for the matching point at every possible location in `im2`, we can use\n",
    "$F$ and simply search over the set of pixels that lie along the epipolar line (recall that the\n",
    "epipolar line passes through a single point in `im2` which corresponds to the point $(x_1, y_1)$ in\n",
    "`im1`).\n",
    "\n",
    "There are various possible ways to compute the window similarity, but simple methods such as the Euclidean or Manhattan distances between the intensity of the pixels should suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "93890876dfec44ff7c88d737d9c29c12",
     "grade": false,
     "grade_id": "cell-cd6040d489714dc2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "If you want to experiment with this:\n",
    "- Try various window sizes.\n",
    "- It may help to use a Gaussian weighting of the window, so that the center has greater influence than the periphery.\n",
    "- Since the two images only differ by a small amount, it might be beneficial to consider matches for which the distance from $(x_1, y_1)$ to $(x_2, y_2)$ is small.\n",
    "\n",
    "To help you test your `epipolarCorrespondence`, we have included a helper function `epipolarMatchGUI`,\n",
    "which takes in two images and the fundamental matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _epipoles(E):\n",
    "    U, S, V = np.linalg.svd(E)\n",
    "    e1 = V[-1, :]\n",
    "    U, S, V = np.linalg.svd(E.T)\n",
    "    e2 = V[-1, :]\n",
    "    return e1, e2\n",
    "\n",
    "def epipolarMatchGUI(I1, I2, F):\n",
    "    e1, e2 = _epipoles(F)\n",
    "\n",
    "    sy, sx, _ = I2.shape\n",
    "\n",
    "    f, [ax1, ax2] = plt.subplots(1, 2, figsize=(12, 9))\n",
    "    ax1.imshow(I1)\n",
    "    ax1.set_title('Select a point in this image')\n",
    "    ax1.set_axis_off()\n",
    "    ax2.imshow(I2)\n",
    "    ax2.set_title('Verify that the corresponding point \\n is on the epipolar line in this image')\n",
    "    ax2.set_axis_off()\n",
    "\n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "\n",
    "    def onclick(event, pts1, pts2):\n",
    "        x1 = event.xdata\n",
    "        y1 = event.ydata\n",
    "\n",
    "        if not x1 or not y1:\n",
    "            return\n",
    "\n",
    "        v = np.array([x1, y1, 1])\n",
    "        l = F.dot(v)\n",
    "        s = np.sqrt(l[0]**2+l[1]**2)\n",
    "\n",
    "        if s==0:\n",
    "            error('Zero line vector in epipolarMatchGUI')\n",
    "\n",
    "        l = l/s\n",
    "\n",
    "        if l[0] != 0:\n",
    "            ye = sy-1\n",
    "            ys = 0\n",
    "            xe = -(l[1] * ye + l[2])/l[0]\n",
    "            xs = -(l[1] * ys + l[2])/l[0]\n",
    "        else:\n",
    "            xe = sx-1\n",
    "            xs = 0\n",
    "            ye = -(l[0] * xe + l[2])/l[1]\n",
    "            ys = -(l[0] * xs + l[2])/l[1]\n",
    "\n",
    "        ax1.plot(x1, y1, '*', MarkerSize=6, linewidth=2)\n",
    "        ax2.plot([xs, xe], [ys, ye], linewidth=2)\n",
    "        plt.draw()\n",
    "\n",
    "        # draw points\n",
    "        x2, y2 = sub.epipolarCorrespondence(I1, I2, F, x1, y1)\n",
    "\n",
    "        # Add error handler. Could return None, if input point is not desired\n",
    "        # (e.g. if it is at the corner, we will not be able to extract image patch for comparison)\n",
    "        if not y1 or not y2:\n",
    "            return\n",
    "\n",
    "        ax2.plot(x2, y2, 'ro', MarkerSize=8, linewidth=2)\n",
    "        plt.draw()\n",
    "\n",
    "        pts1.append([x1, y1])\n",
    "        pts2.append([x2, y2])\n",
    "\n",
    "    f.canvas.mpl_connect('button_press_event', lambda event: onclick(event, pts1, pts2))\n",
    "    plt.show()\n",
    "\n",
    "    return (np.stack(pts1), np.stack(pts2)) if len(pts1) > 0 else ([], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epipolarCorrespondence(im1, im2, F, x1, y1, window_size = 11, sigma = 5, search_range = 40):\n",
    "    # set the size of the window\n",
    "    x1, y1 = int(round(x1)), int(round(y1))\n",
    "    center = window_size // 2\n",
    "\n",
    "    # create gaussian weight matrix\n",
    "    mask = np.ones((window_size, window_size)) * center\n",
    "    mask = np.repeat(np.array([range(window_size)]), window_size,\n",
    "                     axis=0) - mask\n",
    "    mask = np.sqrt(mask**2 + np.transpose(mask)**2)\n",
    "    weight = np.exp(-0.5 * (mask**2) / (sigma**2))\n",
    "    weight /= np.sum(weight)\n",
    "\n",
    "    if len(im1.shape) > 2:\n",
    "        weight = np.repeat(np.expand_dims(weight, axis=2),\n",
    "                           im1.shape[-1],\n",
    "                           axis=2)\n",
    "\n",
    "    # get the epipolar line\n",
    "    p = np.array([[x1], [y1], [1]])\n",
    "    l2 = np.dot(F, p)\n",
    "\n",
    "    # get the patch around the pixel in image1\n",
    "    patch1 = im1[y1 - center:y1 + center + 1, x1 - center:x1 + center + 1]\n",
    "    # get the points on the epipolar line\n",
    "    h, w, _ = im2.shape\n",
    "    Y = np.array(range(y1 - search_range, y1 + search_range))\n",
    "    X = np.round(-(l2[1] * Y + l2[2]) / l2[0]).astype(int)\n",
    "    valid = (X >= center) & (X < w - center) & (Y >= center) & (Y < h - center)\n",
    "    X, Y = X[valid], Y[valid]\n",
    "\n",
    "    min_dist = None\n",
    "    x2, y2 = None, None\n",
    "    for i in range(len(X)):\n",
    "        # get the patch around the pixel in image2\n",
    "        patch2 = im2[Y[i] - center:Y[i] + center + 1,\n",
    "                     X[i] - center:X[i] + center + 1]\n",
    "        # calculate the distance\n",
    "        dist = np.sum((patch1 - patch2)**2 * weight)\n",
    "        if min_dist is None or dist < min_dist:\n",
    "            min_dist = dist\n",
    "            x2, y2 = X[i], Y[i]\n",
    "\n",
    "    return x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP1, SP2 = [], []\n",
    "for i in range(x1.shape[0]):\n",
    "    x2, y2 = epipolarCorrespondence(im1, im2, F8, x1[i], y1[i])\n",
    "    SP1.append([x1[i], y1[i]])\n",
    "    SP2.append([x2, y2])\n",
    "SP1 = np.array(SP1)\n",
    "SP2 = np.array(SP2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a566e4db210d5580598aaed5ef6363d3",
     "grade": false,
     "grade_id": "cell-0bb51e69fd953780",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q4\"></a>\n",
    "#### Triangulation [20 points]\n",
    "\n",
    "Write a function to triangulate a set of 2D coordinates in the image to a set of 3D points\n",
    "where `pts1` and `pts2` are the $N \\times 2$ matrices with the 2D image coordinates and `w` is an \n",
    "$N \\times 3$ matrix with the corresponding 3D points per row. `C1` and `C2` are the $3 \\times 4$ \n",
    "camera matrices. Remember that you will need to multiply the given intrinsics matrices with your\n",
    "solution for the canonical camera matrices to obtain the final camera matrices. Various methods exist\n",
    "for triangulation - we will be using least squares here using **SVD**.\n",
    "\n",
    "The function will be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]])\n",
    "C1 = np.dot(K1, M1)\n",
    "\n",
    "M2s = camera2(E)\n",
    "M2, P = None, None\n",
    "\n",
    "# for i in range(M2s.shape[-1]):\n",
    "#     M2_ = M2s[:, :, i]\n",
    "#     C2_ = np.dot(K2, M2_)\n",
    "#     w, err = triangulate(C1, SP1, C2_, SP2)\n",
    "# \n",
    "#     if np.min(w[:, -1]) > 0:\n",
    "#         M2 = M2_\n",
    "#         P = w\n",
    "#         break\n",
    "# \n",
    "# C2 = np.dot(K2, M2)\n",
    "# P, err = triangulate(C1, SP1, C2, SP2)\n",
    "# print(\"Triangulation error:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f5862424316bc11e863e8806d3526d0b",
     "grade": false,
     "grade_id": "cell-ba60a85278f59a91",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "def triangulate(C1, pts1, C2, pts2):\n",
    "    x1, y1 = pts1[:, 0], pts1[:, 1]\n",
    "    x2, y2 = pts2[:, 0], pts2[:, 1]\n",
    "    A1 = np.vstack(\n",
    "        (C1[0, 0] - C1[2, 0] * x1, C1[0, 1] - C1[2, 1] * x1,\n",
    "         C1[0, 2] - C1[2, 2] * x1, C1[0, 3] - C1[2, 3] * x1)).transpose()\n",
    "    A2 = np.vstack(\n",
    "        (C1[1, 0] - C1[2, 0] * y1, C1[1, 1] - C1[2, 1] * y1,\n",
    "         C1[1, 2] - C1[2, 2] * y1, C1[1, 3] - C1[2, 3] * y1)).transpose()\n",
    "    A3 = np.vstack(\n",
    "        (C2[0, 0] - C2[2, 0] * x2, C2[0, 1] - C2[2, 1] * x2,\n",
    "         C2[0, 2] - C2[2, 2] * x2, C2[0, 3] - C2[2, 3] * x2)).transpose()\n",
    "    A4 = np.vstack(\n",
    "        (C2[1, 0] - C2[2, 0] * y2, C2[1, 1] - C2[2, 1] * y2,\n",
    "         C2[1, 2] - C2[2, 2] * y2, C2[1, 3] - C2[2, 3] * y2)).transpose()\n",
    "\n",
    "    # calculate the 3D coordinates for each point\n",
    "    N = pts1.shape[0]\n",
    "    w = np.zeros((N, 3)) #TODO\n",
    "    for ind in range(N):\n",
    "        A = np.vstack((A1[ind, :], A2[ind, :], A3[ind, :], A4[ind, :]))\n",
    "        _, _, vh = np.linalg.svd(A) #TODO\n",
    "        p = vh[-1, :]\n",
    "        w[ind, :] = p[:3] / p[-1] #TODO\n",
    "\n",
    "    # project to 2D points\n",
    "    W = np.hstack((w, np.ones((N, 1))))\n",
    "    err = 0\n",
    "    for i in range(N):\n",
    "        proj1 = np.dot(C1, np.transpose(W[i, :]))\n",
    "        proj2 = np.dot(C2, np.transpose(W[i, :]))\n",
    "        proj1 = np.transpose(proj1[:2] / proj1[-1])\n",
    "        proj2 = np.transpose(proj2[:2] / proj2[-1])\n",
    "        # compute error\n",
    "        err += np.sum((proj1 - pts1[i])**2 + (proj2 - pts2[i])**2)\n",
    "\n",
    "    return w, err\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "53b39721a855953a716005a43e4196a5",
     "grade": true,
     "grade_id": "cell-18b88c7a262ac27c",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(M2s.shape[-1]):\n",
    "    M2_ = M2s[:, :, i]\n",
    "    C2_ = np.dot(K2, M2_)\n",
    "    w, err = triangulate(C1, SP1, C2_, SP2)\n",
    "\n",
    "    if np.min(w[:, -1]) > 0:\n",
    "        M2 = M2_\n",
    "        P = w\n",
    "        break\n",
    "\n",
    "C2 = np.dot(K2, M2)\n",
    "P, err = triangulate(C1, SP1, C2, SP2)\n",
    "print(\"Triangulation error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5b7a01480ca370824003afd11f7012d5",
     "grade": false,
     "grade_id": "cell-fc17de5a1b622ccd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let us now visualize the result in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(P[:, 0], P[:, 1], P[:, 2], c='r', marker='.')\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2c6510ea408104624e74af5f055acf01",
     "grade": false,
     "grade_id": "cell-bd20521ae8f6ce79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"part-2\"></a>\n",
    "## Camera calibration and  3D reconstruction\n",
    "\n",
    "In this part of the assignment, we will not be given the camera intrinsic matrices. Instead we will use images of a checkerboard pattern captured by two cameras to obtain the camera intrinsic matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "09e91724598f310f0022d3465a571d15",
     "grade": false,
     "grade_id": "cell-9ae0e5afcf37a690",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let us first start by visualizing the two images we will be working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "im1 = cv2.imread(str(root / \"data\" / \"cam1.jpg\"))\n",
    "im2 = cv2.imread(str(root / \"data\" / \"cam2.jpg\"))\n",
    "M = np.max(im1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(cv2.cvtColor(im1, cv2.COLOR_BGR2RGB))\n",
    "ax2.imshow(cv2.cvtColor(im2, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "20f88268923eb9ede3b1db597343f146",
     "grade": false,
     "grade_id": "cell-1cf4c4094f7b8e1a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Did you notice the part where we used the `cvtColor` function with the argument `COLOR_BGR2RGB`? It is because we used the OpenCV library (`cv2`) to load the image and OpenCV uses Blue-Green-Red (BGR) as the order of the 3 channels of the color image when it loads it from disk. Matplotlib, on the other hand, expects RGB and so we have to convert from BGR to RGB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6ee0ebb0f1bbeaf77fb01f94054bfc13",
     "grade": false,
     "grade_id": "cell-840cca5dd3f3a29f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In order to perform camera calibration, we will be using images of a checkerboard pattern captured by the two cameras. Let us load the checkerboard images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "img1x = [cv2.imread(str(img)) for img in glob(str(root / \"data\" / \"1x\" / \"*.jpg\"))]\n",
    "img2x = [cv2.imread(str(img)) for img in glob(str(root / \"data\" / \"2x\" / \"*.jpg\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(cv2.cvtColor(img1x[0], cv2.COLOR_BGR2RGB))\n",
    "ax2.imshow(cv2.cvtColor(img2x[0], cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cde812a7e84a444edf3a87c2b92700e1",
     "grade": false,
     "grade_id": "cell-46005664e6aea283",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let us now define the dimensions of checkerboard. We only use points that lie inside the checkerboard and not the edge points because of how the OpenCV function that we will be using works. We also define a criteria that will tell the OpenCV subpixel refining algorithm when to stop. `objpoints1x` is a vector to store vectors of 3D points for each checkerboard image in the `1x` folder. We will use `objp` for this which we populate here for you. `imgpoints1x` is a vector to store vectors of 2D points for each checkerboard image in the `1x` folder which we will get from OpenCV. Similarly for images in the `2x` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKERBOARD = (6,8)\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 0.1)\n",
    "\n",
    "objpoints1x, objpoints2x = [], []\n",
    "imgpoints1x, imgpoints2x = [], []\n",
    "\n",
    "# world coordinates for 3D points\n",
    "objp = np.zeros((1, CHECKERBOARD[0] * CHECKERBOARD[1], 3), np.float32)\n",
    "objp[0,:,:2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4e43a26df770bfc44940cb9708477c05",
     "grade": false,
     "grade_id": "cell-44bad5ced6b9e0c2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q5\"></a>\n",
    "## Checkerboard pattern detection and Camera Calibration [15 points]\n",
    "\n",
    "- Use the OpenCV function [`findChessboardCorners`](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga93efa9b0aa890de240ca32b11253dd4a) to find the checkerboard pattern in every image in the `1x` folder and append them to `imgpoints1x`. For every $6 \\times 8 = 48$ points you append to imgpoints1x, append `objp` to `objpoints1x` as well. Do the same with `imgpoints2x` and `objpoints2x`.\n",
    "\n",
    "- Use the OpenCV function [`cornerSubPix`](https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga354e0d7c86d0d9da75de9b9701a9a87e) to refine the detected points further. Play with the criteria defination above till you get a result you are happy with.\n",
    "\n",
    "- You can use the OpenCV function [`drawChessboardCorners`](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga6a10b0bb120c4907e5eabbcd22319022) to visualize the output of the `findChessboardCorners` function.\n",
    "\n",
    "- Use the OpenCV function [`calibrateCamera`](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga3207604e4b1a1758aa66acb6ed5aa65d) to use the 4 arrays above to get the intrinsic matrices $\\boldsymbol{K}_1$ and $\\boldsymbol{K}_2$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "611ce1ffc4ea7c5dfbb6f9786d78c8cf",
     "grade": false,
     "grade_id": "cell-1f0b9323cefbb49a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 500, 0.0001)\n",
    "objpoints1x, objpoints2x = [], []\n",
    "imgpoints1x, imgpoints2x = [], []\n",
    "\n",
    "for img in img1x:\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, corners = cv2.findChessboardCorners(img, CHECKERBOARD, cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE)\n",
    "    if ret == True:\n",
    "        objpoints1x.append(objp)\n",
    "        corners2 = cv2.cornerSubPix(gray, corners, (11,11),(-1,-1), criteria)\n",
    "        imgpoints1x.append(corners2)\n",
    "        \n",
    "for img in img2x:\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, corners = cv2.findChessboardCorners(img, CHECKERBOARD, cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE)\n",
    "    if ret == True:\n",
    "        objpoints2x.append(objp)\n",
    "        corners2 = cv2.cornerSubPix(gray, corners, (11,11),(-1,-1), criteria)\n",
    "        imgpoints2x.append(corners2)\n",
    "\n",
    "#TODO: uncomment these lines and replace the 'xxxxxxx' terms with the object points and image points for each image \n",
    "#      to obtain the intrinsic matrices K_1 and K_2. The two \"None\" terms in each line should remain.        \n",
    "_, K1, _, _, _ = cv2.calibrateCamera(objpoints1x, imgpoints1x, img1x[0].shape[:-1][::-1], None, None)\n",
    "_, K2, _, _, _ = cv2.calibrateCamera(objpoints2x, imgpoints2x, img2x[0].shape[:-1][::-1], None, None)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "da0884020570088c7596d7286999bc4c",
     "grade": true,
     "grade_id": "cell-65dbcee5a388db30",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fb1dbd5c6a32796bee270e76e0f9ee9c",
     "grade": false,
     "grade_id": "cell-03514870341e49aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"q6\"></a>\n",
    "## Feature descriptor matching and Fundamental Matrix estimation [15 points]\n",
    "\n",
    "Let us implement the rest of the code, similar to the previous parts of this assignment.\n",
    "For this, we need to get point correspondences between the two images, but we do not have\n",
    "that provided either. Let us use a keypoint detector that uses a feature descriptor\n",
    "([ORB](http://www.gwylab.com/download/ORB_2012.pdf)) to get image features and match them\n",
    "between the two images. We will use the `scikit-image` library functions to do this. In a\n",
    "previous assignment you implemented RANSAC to be robust to outliers. We will use RANSAC\n",
    "here again as the feature matching is inherently noisy and a voting algorithm like RANSAC\n",
    "helps a lot. We will use the built-in `scikit-image` function `ransac` instead of implementing it \n",
    "ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import ransac\n",
    "from skimage.feature import ORB, match_descriptors, plot_matches\n",
    "from skimage.transform import FundamentalMatrixTransform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ec41681e7465365181dd061e4eb213cf",
     "grade": false,
     "grade_id": "cell-e0c44c36bf8e8d5a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "- Use [`skimage.feature.ORB`](https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.ORB)\n",
    "and [`skimage.feature.match_descriptors`](https://scikit-image.org/docs/stable/api/skimage.feature.html#skimage.feature.match_descriptors)\n",
    "to find and match feature descriptors.\n",
    "- Use [`skimage.measure.ransac`](https://scikit-image.org/docs/dev/api/skimage.measure.html#ransac)\n",
    "with appropriate arguments to run RANSAC with the \n",
    "[`skimage.transform.FundamentalMatrixTransform`](https://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.FundamentalMatrixTransform)\n",
    "to estimate the Fundamental Matrix directly.\n",
    "Note: If you use the `FunamentalMatrixTransform` with skimage's `ransac` function, the model returned alread has the fundamental matrix as parameter.\n",
    "- You can use [`skimage.feature.plot_matches`](https://scikit-image.org/docs/stable/api/skimage.feature.html#plot-matches)\n",
    "to visualize the matching before or after running RANSAC and getting inliers.\n",
    "- Calculate the Fundamental and Essential Matrices and store them in `F8` and `E` respectively. You can use functions you have defined earlier in the assignment to achieve this along with the intrinsic matrices you just computed.\n",
    "- Play with the arguments to the keypoint detection and matching calls as well as the `ransac` call to get good inliers and thus accurate estimates of Fundamental and Essential matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e07c85c5e65ee12c076aeeaf47971fe8",
     "grade": false,
     "grade_id": "cell-aea5b7dd9a9429d3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### YOUR ANSWER BELOW\n",
    "\n",
    "### YOUR SOLUTION HERE\n",
    "np.random.seed(0) #for repeatable random values so your results can be compared for testing\n",
    "\n",
    "bim1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "bim2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "detector1 = ORB(n_scales=10, n_keypoints=1000) #try different values here\n",
    "detector2 = ORB(n_scales=10, n_keypoints=1000) #try different values here\n",
    "detector1.detect_and_extract(bim1)\n",
    "detector2.detect_and_extract(bim2)\n",
    "matches = match_descriptors(detector1.descriptors, detector2.descriptors)\n",
    "points1 = detector1.keypoints\n",
    "points2 = detector2.keypoints\n",
    "model, inliers = ransac((points1[matches[:, 0]], points2[matches[:, 1]]),\n",
    "                        FundamentalMatrixTransform,\n",
    "                        min_samples=8,           #try different values here\n",
    "                        residual_threshold=0.3,  #try different values here\n",
    "                        max_trials=10000)        #try different values here\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1)\n",
    "plt.gray()\n",
    "plot_matches(ax,\n",
    "             im1,\n",
    "             im2,\n",
    "             points1,\n",
    "             points2,\n",
    "             matches[inliers],\n",
    "             only_matches=True)\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"Inlier correspondences\")\n",
    "plt.show()\n",
    "\n",
    "points1 = np.stack(\n",
    "    [points1[matches[inliers, 0], 1], points1[matches[inliers, 0], 0]], 1)\n",
    "points2 = np.stack(\n",
    "    [points2[matches[inliers, 1], 1], points2[matches[inliers, 1], 0]], 1)\n",
    "F8 = model.params\n",
    "\n",
    "#TODO\n",
    "#the fundamental matrix, intrinsic matrix 1, and intrinsic matrix 2\n",
    "E = essentialMatrix(F8, K1, K2) #essentialMatrix(xxxxxxx, xxxxxxx, xxxxxxx)\n",
    "\n",
    "print(F8) #view your fundamental matrix\n",
    "print(E)  #view your essential matrix\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9f937c14946595b5a873cd49d9c8c7f8",
     "grade": true,
     "grade_id": "cell-ec75b915411da2a7",
     "locked": true,
     "points": 15,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c17ad623c4c9fc47dbd175ed1b14d489",
     "grade": false,
     "grade_id": "cell-506936cfbcf28b8c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Let us now run the rest of the script to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.load(root / \"data\" / \"camera_coords.npz\")\n",
    "x1, y1 = coords['x1'][:, 0]/2.5, coords['y1'][:, 0]/2.5\n",
    "SP1, SP2 = [], []\n",
    "for i in range(x1.shape[0]):\n",
    "    x2, y2 = epipolarCorrespondence(im1, im2, F8, x1[i], y1[i], window_size = 41, search_range = 70)\n",
    "    SP1.append([x1[i], y1[i]])\n",
    "    SP2.append([x2, y2])\n",
    "SP1 = np.array(SP1)\n",
    "SP2 = np.array(SP2)\n",
    "M1 = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]])\n",
    "C1 = np.dot(K1, M1)\n",
    "M2s = camera2(E)\n",
    "\n",
    "M2, P = None, None\n",
    "\n",
    "for i in range(M2s.shape[-1]):\n",
    "    M2_ = M2s[:, :, i]\n",
    "    C2_ = np.dot(K2, M2_)\n",
    "    w, err = triangulate(C1, SP1, C2_, SP2)\n",
    "\n",
    "    if np.min(w[:, -1]) > 0:\n",
    "        M2 = M2_\n",
    "        P = w\n",
    "        break\n",
    "\n",
    "C2 = np.dot(K2, M2)\n",
    "P, err = triangulate(C1, SP1, C2, SP2)\n",
    "print(\"Triangulation error:\", err)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(P[:, 0], P[:, 1], P[:, 2], c='r', marker='.')\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d3b19a94aca384c2f57e26c425ff07b3",
     "grade": false,
     "grade_id": "cell-c7bfea3235d79454",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<a name=\"part-3\"></a>\n",
    "## Challenge Problems\n",
    "\n",
    "1. Notice how the Triangulation error in this case is much higher compared to last time\n",
    "when the point correspondences and camera intrinsics were known. This can be fixed using\n",
    "something known as [Bundle Adjustment](https://en.wikipedia.org/wiki/Bundle_adjustment)\n",
    "which is outside the scope of this assignment. Feel free to read it up and post your \n",
    "results/findings in the forum.\n",
    "\n",
    "2. The images used here are of a complex object (a DSLR camera). We provide a simpler\n",
    "object pair in the assignment folder named `mug1.jpg` and `mug2.jpg` captured using the\n",
    "same pair of cameras. Try to get 3D reconstruction results similar to what what we got\n",
    "here on that image pair. Post your results in the forums.\n",
    "Note: 2D coordinates are not provided for the pair. You will have to select some points\n",
    "that would best describe the object. You can use the matplotlib function\n",
    "[`ginput`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.ginput.html)\n",
    "to get points on an image interactively.\n",
    "\n",
    "3. If you have a smartphone with multiple cameras, try to capture image pairs with the\n",
    "different cameras and 3D reconstruct objects in the image pairs. You will have to capture\n",
    "the checkerboard pattern with the cameras similar to what we did above to get camera\n",
    "intrinsics. Post your 3D reconstructed objects in the forums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
